{
  "hash": "5a39f9fdf1d6da9ce8a5c2d660b152e2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parquet is the new TSV\"\ndescription: \"Larger-than-memory data access with .parquet files\"\ndate: 2025-06-05\nauthor: \n  - name: Fabrício Almeida-Silva\n    orcid: 0000-0002-5314-2964\ndraft: false\nimage: fast.gif\ncategories:\n  - bioinformatics\n  - database\nexecute: \n  freeze: true\n---\n\n\n## Motivation\n\nFor programming languages that store data in memory (e.g. R and Python),\nworking with large data files can be a problem. For example, if you need\nto subset a few rows from a large table stored in a 5 GB TSV/CSV file,\nyou will first need to read the entire table, then subset the rows you want.\nIf your data is larger than your memory capacity, reading the entire data set\nis not possible. [Apache Parquet](https://parquet.apache.org/) is a \ncolumn-oriented file format (similar to TSV/CSV) designed for efficient \ndata storage and retrieval, and it can be used by packages such as\n[arrow](https://arrow.apache.org/docs/r/) to analyze larger-than-memory\ndata sets. Here, I will demonstrate the advantages of storing large data\nin Parquet files (compared to TSV/CSV files), and benchmark data retrieval\nusing Parquet files and other alternatives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(duckdb)\n```\n:::\n\n\n## Example data\n\n[PLAZA](https://bioinformatics.psb.ugent.be/plaza.dev/instances/dicots_05/)\nis a database for plant comparative genomics data. Among many important \nfeatures and data resources, PLAZA provides orthologous relationships for\nplant genes using differnent 'orthology types'. \nHere, I will use orthologous genes obtained with the \nbest-hit-and-inparalogs (BHI) type. This is a large (3 GB) CSV file\ncontaining the orthologs for all genes in all species in PLAZA Dicots 5.0.\nFor example, if you have a gene of interest in *Arabidopsis thaliana*, you\ncan use this file to find the corresponding (orthologous) gene(s) in other\nplants for comparative studies.\n\nLet's first download the file to a temporary directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download file\noptions(timeout = 1e6) # download might take a while\n\ncsv_file <- file.path(tempdir(), \"plaza_dicots_bhif.csv.gz\")\ndownload.file(\n    url = \"https://ftp.psb.ugent.be/pub/plaza/plaza_public_dicots_05/IntegrativeOrthology/integrative_orthology.BHIF.csv.gz\",\n    destfile = csv_file\n)\n```\n:::\n\n\n## From CSV to Parquet\n\nTo create a Parquet file, we could read the CSV file to the R session \nand export it as .parquet file using the *[arrow](https://CRAN.R-project.org/package=arrow)* or\n*[nanoparquet](https://CRAN.R-project.org/package=nanoparquet)* packages. This works well if you have\naccess to an HPC or a powerful server. If you don't have access to a machine\nwith more memory, don't panic: you can use the *[duckdb](https://CRAN.R-project.org/package=duckdb)*\npackage to create a Parquet file from a CSV file without having to load it \nfirst. This can be achieved with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Directly convert CSV to Parquet using {duckdb}\ncon <- dbConnect(duckdb())\nparquet_file <- file.path(tempdir(), \"plaza_dicots_bhif.parquet\")\n\nquery <- paste0(\"\n  COPY (SELECT * FROM '\", csv_file, \"') \n  TO '\", parquet_file, \"' (FORMAT PARQUET)\n\")\ndbExecute(con, query)\ndbDisconnect(con)\n```\n:::\n\n\nIn terms of storage, a Parquet file is comparable to a gzipped CSV file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect file sizes\ndata.frame(\n    Format = c(\"CSV\", \"Parquet\"),\n    Size = fs::file_size(c(csv_file, parquet_file)),\n    row.names = NULL\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Format  Size\n1     CSV 3.03G\n2 Parquet 3.67G\n```\n\n\n:::\n:::\n\n\n## Larger-than-memory data access with __{arrow}__\n\nOnce you have tabular data in a Parquet file, you can use \nthe *[arrow](https://CRAN.R-project.org/package=arrow)* package to 'connect' to the file,\nperform some data transformation (e.g., filter rows, subset columns, \nsummarize data by groups, etc) using *[tidyverse](https://CRAN.R-project.org/package=tidyverse)* verbs, \nand read only the output of the data transformation. \nIf your familiar with SQL, this is similar to performing SQL queries without\nloading the data in memory.\n\nTo demonstrate how this works, we will extract orthologs (i.e., \nbest-hits-and-inparalogs) of the gene *AT2G14610*, which encodes a \npathogenesis-related protein 1 (PR-1) in *Arabidopsis thaliana*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Connect to the Parquet file\nbhi <- arrow::open_dataset(parquet_file)\nbhi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 1 Parquet file\n4 columns\n#query_gene: string\nquery_species: string\northologous_gene: string\northologous_species: string\n```\n\n\n:::\n:::\n\n\nOnce we connect to the Parquet file, we can see that it contains\na table with four columns named `#query_gene`, `query_species`, \n`orthologous_gene`, and `orthologous_species`. Now, we will filter the table\nto keep only rows that have 'AT2G14610' in column `#query_gene`, and collect\nthe results to the R session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract best-hits-and-inparalogs of 'AT2G14610'\npr1_orthologs <- bhi |>\n    filter(`#query_gene` == \"AT2G14610\") |>\n    collect()\n\nhead(pr1_orthologs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  `#query_gene` query_species orthologous_gene               orthologous_species\n  <chr>         <chr>         <chr>                          <chr>              \n1 AT2G14610     ath           AagrBONN_evm.TU.Sc2ySwM_228.2… aag                \n2 AT2G14610     ath           Aa31LG2G16880                  aar                \n3 AT2G14610     ath           Aa31LG2G16870                  aar                \n4 AT2G14610     ath           Atru.chr5.2212                 acertr             \n5 AT2G14610     ath           Actinidia00340                 ach                \n6 AT2G14610     ath           AL3G46130                      aly                \n```\n\n\n:::\n:::\n\n\nBrilliant, isn't it? Using Parquet files, we can seamlessly subset a table\nthat is too large to fit in memory, solving the problem of larger-than-memory\ndata analysis.\n\n## Benchmark\n\nNow, you might be asking yourself: \n\n> Do I need Parquet files? Why not extract the rows I want from the CSV file using Bash code?\n\nThat is indeed possible. In your terminal, you can use Bash code to read the\nCSV file line by line and search for rows that match our condition (i.e.,\n'AT2G14610' in column `#query_gene`). The Bash code would look something like\nthis:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nzcat file.csv.gz | grep -E '^#query|^AT2G14610' > filtered.csv\n```\n:::\n\n\nLet's compare the performance of the Bash-based approach with the\n*[arrow](https://CRAN.R-project.org/package=arrow)*-based approach. Of note, the .csv.gz file\nfrom PLAZA is actually a TSV file, not CSV, but that doesn't impact \nperformance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wrapper function for the Bash-based approach\nfilter_bash <- function() {\n    outfile <- file.path(tempdir(), \"output.tsv\")\n    args <- c(csv_file, \" | grep -E '^#query|^AT2G14610' > \", outfile)\n    system2(\"zcat\", args = args)\n    \n    df <- read_tsv(outfile, show_col_types = FALSE) |> as.data.frame()\n    return(df)\n}\n\n# Benchmark\nbnch <- bench::mark(\n    arrow = bhi |>\n        filter(`#query_gene` == \"AT2G14610\") |>\n        collect() |>\n        as.data.frame(),\n    bash = filter_bash()\n)\n\nbnch\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 arrow        29.8ms   33.8ms   27.2       58.6KB     4.53\n2 bash          58.3s    58.3s    0.0171     1.8MB     0   \n```\n\n\n:::\n:::\n\n\nThe benchmark shows that the *[arrow](https://CRAN.R-project.org/package=arrow)*-based approach\n(using Parquet files) is **much** faster than the Bash approach \n(milliseconds vs one minute!). Hence, developers and maintainers of databases\nthat provide users with large data files should consider providing\nParquet files besides traditional CSV/TSV files. In the era of machine learning,\nAI, and large data, I believe this will make data analysis workflows much \nfaster and more efficient.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}